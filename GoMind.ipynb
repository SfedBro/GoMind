{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611eac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal AlphaZero-style skeleton for Go (self-play + MCTS + neural net)\n",
    "# Dependencies: torch, numpy\n",
    "\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import copy\n",
    "from typing import List, Tuple, Optional, Set, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f559e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1) Basic Go board\n",
    "# ---------------------------\n",
    "EMPTY = 0\n",
    "BLACK = 1\n",
    "WHITE = 2\n",
    "# Opponent's color\n",
    "def opponent(player: int) -> int:\n",
    "    return BLACK if player == WHITE else WHITE\n",
    "\n",
    "class Board:\n",
    "    def __init__(self, size: int = 9, komi: float = None, history_len: int = 8):\n",
    "        if komi is None:\n",
    "            match (size): # New switch case notation\n",
    "                case (19):\n",
    "                    komi = 5.5\n",
    "                case (13):\n",
    "                    komi = 4.5\n",
    "                case(9):\n",
    "                    komi = 3.5\n",
    "                case (_):\n",
    "                    komi = 1.5\n",
    "            # end match\n",
    "        self.size = size\n",
    "        self.komi = komi\n",
    "        self.grid = np.zeros((size, size), dtype=np.int8)\n",
    "        self.history: Set[Tuple[Tuple[int, ...], int]] = set()\n",
    "        self.past_states: deque = deque(maxlen=history_len)\n",
    "        # counters captures\n",
    "        self.black_captures = 0\n",
    "        self.white_captures = 0\n",
    "        # ko point: (x,y) or None\n",
    "        self.ko: Optional[Tuple[int, int]] = None\n",
    "        # number of consecutive passes\n",
    "        self.pass_count = 0\n",
    "        # initially add starting position with Black to move\n",
    "        self._add_history(player=BLACK)\n",
    "\n",
    "    def copy(self) -> 'Board':\n",
    "        b = Board(self.size, self.komi, history_len=self.past_states.maxlen)\n",
    "        b.grid = self.grid.copy()\n",
    "        b.history = set(self.history)\n",
    "        if len(self.past_states):\n",
    "            # copy elements safely\n",
    "            copied = (s.copy() if isinstance(s, np.ndarray) else s for s in self.past_states)\n",
    "            b.past_states = deque(copied, maxlen=self.past_states.maxlen)\n",
    "        else:\n",
    "            b.past_states = deque(maxlen=self.past_states.maxlen)\n",
    "        # copy capture counters and ko state\n",
    "        b.black_captures = self.black_captures\n",
    "        b.white_captures = self.white_captures\n",
    "        b.ko = None if self.ko is None else (self.ko[0], self.ko[1])\n",
    "        b.pass_count = self.pass_count\n",
    "        return b\n",
    "\n",
    "    def in_bounds(self, x:int, y:int) -> bool:\n",
    "        return 0 <= x < self.size and 0 <= y < self.size\n",
    "\n",
    "    def neighbors(self, x:int, y:int):\n",
    "        for dx, dy in ((1,0),(-1,0),(0,1),(0,-1)):\n",
    "            nx, ny = x+dx, y+dy\n",
    "            if self.in_bounds(nx, ny):\n",
    "                yield nx, ny\n",
    "\n",
    "    def _board_key(self, player: int) -> Tuple[Tuple[int, ...], int]:\n",
    "        # включает игрока для корректного суперко\n",
    "        return (tuple(self.grid.ravel().tolist()), player)\n",
    "\n",
    "    def _add_history(self, player: int):\n",
    "        self.history.add(self._board_key(player))\n",
    "        # сохраняем копию сетки для NN-плоскостей\n",
    "        self.past_states.append(self.grid.copy())\n",
    "\n",
    "    def get(self, x:int, y:int) -> int:\n",
    "        return int(self.grid[y,x])\n",
    "\n",
    "    def set(self, x:int, y:int, v:int):\n",
    "        self.grid[y,x] = v\n",
    "\n",
    "    def _collect_group(self, x:int, y:int):\n",
    "        color = self.get(x, y)\n",
    "        if color == EMPTY:\n",
    "            return [], set()\n",
    "        visited = set()\n",
    "        stack = [(x,y)]\n",
    "        group = []\n",
    "        liberties = set()\n",
    "        while stack:\n",
    "            cx, cy = stack.pop()\n",
    "            if (cx, cy) in visited:\n",
    "                continue\n",
    "            visited.add((cx, cy))\n",
    "            group.append((cx, cy))\n",
    "            for nx, ny in self.neighbors(cx, cy):\n",
    "                val = self.get(nx, ny)\n",
    "                if val == color and (nx, ny) not in visited:\n",
    "                    stack.append((nx, ny))\n",
    "                elif val == EMPTY:\n",
    "                    liberties.add((nx, ny))\n",
    "        return group, liberties\n",
    "\n",
    "    def _remove_group(self, group: List[Tuple[int, int]]) -> int:\n",
    "        if not group:\n",
    "            return 0\n",
    "        col = self.get(group[0][0], group[0][1])\n",
    "        for x,y in group:\n",
    "            self.set(x,y,EMPTY)\n",
    "        return len(group)\n",
    "\n",
    "    def _play_move_no_checks(self, player:int, move:Optional[Tuple[int,int]]) -> int:\n",
    "        # move: None means pass\n",
    "        if move is None:\n",
    "            self.pass_count += 1\n",
    "            return 0\n",
    "        x,y = move\n",
    "        self.set(x,y, player)\n",
    "        self.pass_count = 0\n",
    "        # capture adjacent enemy groups with zero liberties\n",
    "        removed = 0\n",
    "        to_remove = set()\n",
    "        for nx, ny in self.neighbors(x,y):\n",
    "            if self.get(nx, ny) == opponent(player):\n",
    "                g, libs = self._collect_group(nx, ny)\n",
    "                if len(libs) == 0:\n",
    "                    to_remove.update(g)\n",
    "        for rx, ry in to_remove:\n",
    "            self.set(rx, ry, EMPTY)\n",
    "            removed += 1\n",
    "        if removed:\n",
    "            if player == BLACK:\n",
    "                self.black_captures += removed\n",
    "            else:\n",
    "                self.white_captures += removed\n",
    "        if removed == 1:\n",
    "            removed_list = list(to_remove)\n",
    "            rx, ry = removed_list[0]\n",
    "            # ko — это точка, где сейчас нет камня и все соседи заняты так, что возврат создает повтор\n",
    "            self.ko = (rx, ry)\n",
    "        else:\n",
    "            self.ko = None\n",
    "        return removed\n",
    "\n",
    "    def is_suicide(self, player:int, move:Tuple[int,int]) -> bool:\n",
    "        x,y = move\n",
    "        tmp_grid = self.grid.copy()\n",
    "        tmp_grid[y, x] = player\n",
    "        removed_any = False\n",
    "        # remove opponent dead groups\n",
    "        for nx, ny in self.neighbors(x, y):\n",
    "            if tmp_grid[ny, nx] == opponent(player):\n",
    "                # собрать группу на временной сетке\n",
    "                # реализуем сборку локально, чтобы не менять self\n",
    "                stack = [(nx, ny)]\n",
    "                visited = set()\n",
    "                libs = set()\n",
    "                while stack:\n",
    "                    cx, cy = stack.pop()\n",
    "                    if (cx, cy) in visited:\n",
    "                        continue\n",
    "                    visited.add((cx, cy))\n",
    "                    for sx, sy in ((cx+1, cy), (cx-1, cy), (cx, cy+1), (cx, cy-1)):\n",
    "                        if 0 <= sx < self.size and 0 <= sy < self.size:\n",
    "                            val = tmp_grid[sy, sx]\n",
    "                            if val == opponent(player) and (sx, sy) not in visited:\n",
    "                                stack.append((sx, sy))\n",
    "                            elif val == EMPTY:\n",
    "                                libs.add((sx, sy))\n",
    "                if not libs:\n",
    "                    # эта группа будет снята\n",
    "                    removed_any = True\n",
    "                    # очистим ее на tmp_grid\n",
    "                    for (gx, gy) in visited:\n",
    "                        tmp_grid[gy, gx] = EMPTY\n",
    "        # теперь проверить свободы собственной группы\n",
    "        stack = [(x, y)]\n",
    "        visited = set()\n",
    "        libs = set()\n",
    "        while stack:\n",
    "            cx, cy = stack.pop()\n",
    "            if (cx, cy) in visited:\n",
    "                continue\n",
    "            visited.add((cx, cy))\n",
    "            for sx, sy in ((cx+1, cy), (cx-1, cy), (cx, cy+1), (cx, cy-1)):\n",
    "                if 0 <= sx < self.size and 0 <= sy < self.size:\n",
    "                    val = tmp_grid[sy, sx]\n",
    "                    if val == player and (sx, sy) not in visited:\n",
    "                        stack.append((sx, sy))\n",
    "                    elif val == EMPTY:\n",
    "                        libs.add((sx, sy))\n",
    "        return len(libs) == 0\n",
    "\n",
    "    def is_legal(self, player:int, move:Optional[Tuple[int,int]]) -> bool:\n",
    "        if move is None:\n",
    "            return True\n",
    "        x,y = move\n",
    "        if not self.in_bounds(x,y): return False\n",
    "        if self.get(x,y) != EMPTY: return False\n",
    "        if self.ko is not None and (x, y) == self.ko:\n",
    "            pass\n",
    "        # suicide\n",
    "        if self.is_suicide(player, (x,y)): return False\n",
    "        # superko (simple): simulate and check history\n",
    "        tmp = self.copy()\n",
    "        tmp._play_move_no_checks(player, (x,y))\n",
    "        next_player = opponent(player)\n",
    "        if tmp._board_key(next_player) in self.history:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def legal_moves(self, player:int) -> List[Optional[Tuple[int,int]]]:\n",
    "        moves: List[Optional[Tuple[int, int]]] = []\n",
    "        for y in range(self.size):\n",
    "            for x in range(self.size):\n",
    "                if self.grid[y,x] == EMPTY and self.is_legal(player, (x,y)):\n",
    "                    moves.append((x,y))\n",
    "        moves.append(None)  # pass\n",
    "        if len(moves) > self.size * 0.25: # NO MORE INFINITE PASSES\n",
    "            non_pass_moves = [m for m in moves if m != None]\n",
    "            if non_pass_moves:\n",
    "                moves = non_pass_moves\n",
    "        return moves\n",
    "    \n",
    "    def play_move(self, player: int, move: Optional[Tuple[int, int]]) -> bool:\n",
    "        \"\"\"Попытаться сыграть ход; вернуть True если ход сделан, False если нелегален.\"\"\"\n",
    "        if not self.is_legal(player, move):\n",
    "            return False\n",
    "        # выполняем ход и обновляем историю с учетом следующего игрока\n",
    "        removed = self._play_move_no_checks(player, move)\n",
    "        next_player = opponent(player)\n",
    "        # обновляем history и past_states\n",
    "        self._add_history(next_player)\n",
    "        return True\n",
    "\n",
    "    def game_over(self) -> bool:\n",
    "        return self.pass_count >= 2\n",
    "\n",
    "    def score_chinese(self) -> Tuple[float, float]:\n",
    "        # area scoring (stones + territory)\n",
    "        size = self.size\n",
    "        visited = np.zeros((size, size), dtype=bool)\n",
    "        black_area = int((self.grid == BLACK).sum())\n",
    "        white_area = int((self.grid == WHITE).sum())\n",
    "        from collections import deque as _dq\n",
    "        for y in range(size):\n",
    "            for x in range(size):\n",
    "                if self.get(x, y) != EMPTY or visited[y, x]:\n",
    "                    continue\n",
    "                q = _dq()\n",
    "                q.append((x, y))\n",
    "                visited[y, x] = True\n",
    "                region = [(x, y)]\n",
    "                bordering = set()\n",
    "                while q:\n",
    "                    cx, cy = q.popleft()\n",
    "                    for nx, ny in self.neighbors(cx, cy):\n",
    "                        val = self.get(nx, ny)\n",
    "                        if val == EMPTY and not visited[ny, nx]:\n",
    "                            visited[ny, nx] = True\n",
    "                            q.append((nx, ny))\n",
    "                            region.append((nx, ny))\n",
    "                        elif val in (BLACK, WHITE):\n",
    "                            bordering.add(val)\n",
    "                if len(bordering) == 1:\n",
    "                    owner = next(iter(bordering))\n",
    "                    if owner == BLACK:\n",
    "                        black_area += len(region)\n",
    "                    else:\n",
    "                        white_area += len(region)\n",
    "        # komi к белым\n",
    "        white_area += self.komi\n",
    "        return float(black_area), float(white_area)\n",
    "\n",
    "    def score_japanese(self) -> Tuple[float, float]:\n",
    "        # territory + captures\n",
    "        size = self.size\n",
    "        visited = np.zeros((size, size), dtype=bool)\n",
    "        black_territory = 0\n",
    "        white_territory = 0\n",
    "        from collections import deque as _dq\n",
    "        for y in range(size):\n",
    "            for x in range(size):\n",
    "                if self.get(x, y) != EMPTY or visited[y, x]:\n",
    "                    continue\n",
    "                q = _dq()\n",
    "                q.append((x, y))\n",
    "                visited[y, x] = True\n",
    "                region = [(x, y)]\n",
    "                bordering = set()\n",
    "                while q:\n",
    "                    cx, cy = q.popleft()\n",
    "                    for nx, ny in self.neighbors(cx, cy):\n",
    "                        val = self.get(nx, ny)\n",
    "                        if val == EMPTY and not visited[ny, nx]:\n",
    "                            visited[ny, nx] = True\n",
    "                            q.append((nx, ny))\n",
    "                            region.append((nx, ny))\n",
    "                        elif val in (BLACK, WHITE):\n",
    "                            bordering.add(val)\n",
    "                if len(bordering) == 1:\n",
    "                    owner = next(iter(bordering))\n",
    "                    if owner == BLACK:\n",
    "                        black_territory += len(region)\n",
    "                    else:\n",
    "                        white_territory += len(region)\n",
    "        black_score = black_territory + self.black_captures\n",
    "        white_score = white_territory + self.white_captures + self.komi\n",
    "        return float(black_score), float(white_score)\n",
    "\n",
    "    def winner(self, japanese: bool = False) -> int:\n",
    "        if japanese:\n",
    "            b, w = self.score_japanese()\n",
    "        else:\n",
    "            b, w = self.score_chinese()\n",
    "        return BLACK if b > w else WHITE\n",
    "    \n",
    "    def get_state_planes(self, player: int, history_len: int = 8) -> np.ndarray:\n",
    "        \"\"\"Возвращает плоскости для NN: shape = (C, size, size).\n",
    "        Формат:\n",
    "        - для каждой из последних history_len позиций: 2 плоскости (black, white)\n",
    "        - плоскость to_move (всегда 1.0, если player == BLACK)\n",
    "        - плоскость ko (1 в точке ko)\n",
    "        Итоговое число плоскостей = 2*history_len + 2\"\"\"\n",
    "        hl = min(history_len, self.past_states.maxlen)\n",
    "        planes = []\n",
    "        # собираем последние hl позиций, от старых к новым\n",
    "        # если истории меньше, заполняем нулями\n",
    "        states = list(self.past_states)\n",
    "        pad = hl - len(states)\n",
    "        for _ in range(pad):\n",
    "            planes.append(np.zeros((self.size, self.size), dtype=np.float32)) # black\n",
    "            planes.append(np.zeros((self.size, self.size), dtype=np.float32)) # white\n",
    "        # взять последние hl состояний\n",
    "        for s in states[-hl:]:\n",
    "            black_plane = (s == BLACK).astype(np.float32)\n",
    "            white_plane = (s == WHITE).astype(np.float32)\n",
    "            planes.append(black_plane)\n",
    "            planes.append(white_plane)\n",
    "        # to_move\n",
    "        turn_plane = np.full((self.size, self.size), 1.0 if player == BLACK else 0.0, dtype=np.float32)\n",
    "        planes.append(turn_plane)\n",
    "        # ko\n",
    "        ko_plane = np.zeros((self.size, self.size), dtype=np.float32)\n",
    "        if self.ko is not None:\n",
    "            kx, ky = self.ko\n",
    "            ko_plane[ky, kx] = 1.0\n",
    "        planes.append(ko_plane)\n",
    "        return np.stack(planes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "450d4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2) Neural network (PyTorch)\n",
    "# ---------------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False)\n",
    "        self.gn = nn.GroupNorm(8, out_ch)\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.gn(self.conv(x)))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.gn1 = nn.GroupNorm(8, channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.gn2 = nn.GroupNorm(8, channels)\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.gn1(self.conv1(x)))\n",
    "        out = self.gn2(self.conv2(out))\n",
    "        out = torch.relu(out + x)\n",
    "        return out\n",
    "\n",
    "class AlphaNet(nn.Module):\n",
    "    def __init__(self, board_size=9, in_channels=18, num_res=6, channels=128):\n",
    "        super().__init__()\n",
    "        self.board_size = board_size\n",
    "        self.conv_in = ConvBlock(in_channels, channels)\n",
    "        self.resblocks = nn.Sequential(*[ResidualBlock(channels) for _ in range(num_res)])\n",
    "\n",
    "        # policy head\n",
    "        self.p_conv1 = nn.Conv2d(channels, 32, 1, bias=False)\n",
    "        self.p_gn1 = nn.GroupNorm(8, 32)\n",
    "        self.p_conv2 = nn.Conv2d(32, 2, 1, bias=False)\n",
    "        self.p_gn2 = nn.GroupNorm(2, 2)\n",
    "        self.p_fc = nn.Linear(2 * board_size * board_size, board_size * board_size + 1)  # +1 for pass\n",
    "        # value head\n",
    "        self.v_conv1 = nn.Conv2d(channels, 32, 1, bias=False)\n",
    "        self.v_gn1 = nn.GroupNorm(8, 32)\n",
    "        self.v_fc1 = nn.Linear(32 * board_size * board_size, 128)\n",
    "        self.v_fc2 = nn.Linear(128, 1)\n",
    "    def forward(self, x, legal_mask=None):\n",
    "        out = self.conv_in(x)\n",
    "        out = self.resblocks(out)\n",
    "        # policy\n",
    "        p = torch.relu(self.p_gn1(self.p_conv1(out)))\n",
    "        p = torch.relu(self.p_gn2(self.p_conv2(p)))\n",
    "        p = p.view(p.size(0), -1)\n",
    "        logits = self.p_fc(p)\n",
    "\n",
    "        if legal_mask is not None:\n",
    "            logits = logits.masked_fill(~legal_mask, float(\"-inf\"))\n",
    "\n",
    "        logp = torch.log_softmax(logits, dim=1)\n",
    "        # value\n",
    "        v = torch.relu(self.v_gn1(self.v_conv1(out)))\n",
    "        v = v.view(v.size(0), -1)\n",
    "        v = torch.relu(self.v_fc1(v))\n",
    "        value = torch.tanh(self.v_fc2(v)).squeeze(1)\n",
    "        \n",
    "        return logp, value  # log probabilities and scalar value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "346a66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3) MCTS with PUCT\n",
    "# ---------------------------\n",
    "class MCTSNode:\n",
    "    def __init__(self, prior:float=0.0):\n",
    "        self.prior = float(prior)   # P(a)\n",
    "        self.visit_count = 0        # N\n",
    "        self.value_sum = 0.0        # W\n",
    "        self.children = {}          # action -> MCTSNode\n",
    "        self.lock = threading.Lock() # for parallel updates\n",
    "        self.virtual_loss = 0       # for virtual loss\n",
    "\n",
    "    def q_value(self) -> float:\n",
    "        if self.visit_count == 0:\n",
    "            return 0.0\n",
    "        return self.value_sum / self.visit_count\n",
    "    \n",
    "    def n(self) -> int:\n",
    "        return self.visit_count\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, net:AlphaNet, board_size:int, c_puct:float=1.0, n_simulations:int=80, device='cpu'):\n",
    "        self.net = net\n",
    "        self.board_size = board_size\n",
    "        self.c_puct = c_puct\n",
    "        self.n_simulations = n_simulations\n",
    "        self.device = device\n",
    "        self.prof = {'nn': 0.0, 'copy': 0.0, 'legal': 0.0, 'sim': 0.0}\n",
    "\n",
    "    def run(self, root_board:Board, to_move:int, eps:float=0.25, alpha:float=0.03, temperature: float = 1.0, parallel: bool = False):\n",
    "        root = MCTSNode()\n",
    "        size = self.board_size\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        # evaluate root with network to get priors\n",
    "        state = torch.tensor(root_board.get_state_planes(to_move), dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logp, v = self.net(state.to(self.device))\n",
    "        probs = torch.exp(logp).squeeze(0).cpu().numpy()  # length = H*W + 1\n",
    "        self.prof['nn'] += time.perf_counter() - t0\n",
    "\n",
    "        # build list of legal indices at root\n",
    "        legal = root_board.legal_moves(to_move)\n",
    "        legal_indices: List[int] = []\n",
    "        for mv in legal:\n",
    "            if mv is None:\n",
    "                idx = size*size\n",
    "            else:\n",
    "                idx = mv[1]*size + mv[0]\n",
    "            legal_indices.append(idx)\n",
    "\n",
    "        # mask illegal priors, then renormalize\n",
    "        masked_priors = np.zeros_like(probs, dtype=np.float32)\n",
    "        masked_priors[legal_indices] = probs[legal_indices]\n",
    "        total = masked_priors.sum()\n",
    "        if total > 0:\n",
    "            masked_priors /= total\n",
    "        else:\n",
    "            # fallback uniform over legal\n",
    "            for idx in legal_indices:\n",
    "                masked_priors[idx] = 1.0 / len(legal_indices)\n",
    "        \n",
    "        # add Dirichlet noise for root during self-play\n",
    "        dir_noise = np.random.dirichlet([alpha] * len(legal_indices))\n",
    "        for i, idx in enumerate(legal_indices):\n",
    "            masked_priors[idx] = (1 - eps) * masked_priors[idx] + eps * dir_noise[i]\n",
    "\n",
    "        # create root children with priors \n",
    "        for idx in legal_indices:\n",
    "            root.children[idx] = MCTSNode(prior=float(masked_priors[idx]))\n",
    "\n",
    "        # run simulations\n",
    "        if parallel:\n",
    "            threads = []\n",
    "            for _ in range(self.n_simulations):\n",
    "                th = threading.Thread(target=self._simulate, args=(root_board.copy(), to_move, root))\n",
    "                th.start()\n",
    "                threads.append(th)\n",
    "            for th in threads:\n",
    "                th.join()\n",
    "        else:\n",
    "            for _ in range(self.n_simulations):\n",
    "                board_copy = root_board.copy()\n",
    "                self._simulate(board_copy, to_move, root)\n",
    "\n",
    "        # build policy target pi from visit counts\n",
    "        counts = np.zeros(self.board_size*self.board_size + 1, dtype=np.float32)\n",
    "        for a, node in root.children.items():\n",
    "            counts[a] = node.visit_count\n",
    "        total = counts.sum()\n",
    "        if total <= 0:\n",
    "            # fallback uniform over legal actions\n",
    "            for idx in legal_indices:\n",
    "                counts[idx] = 1.0\n",
    "            total = counts.sum()\n",
    "        pi = counts / total\n",
    "        #return pi  # return visit-probabilities\n",
    "        # temperature: sample or take argmax depending on temperature\n",
    "        if temperature == 0 or temperature < 1e-6:\n",
    "            action = int(np.argmax(pi))\n",
    "            final_pi = np.zeros_like(pi)\n",
    "            final_pi[action] = 1.0\n",
    "            return final_pi\n",
    "        else:\n",
    "            # apply temperature to pi\n",
    "            pi_temp = pi ** (1.0 / temperature)\n",
    "            pi_temp_sum = pi_temp.sum()\n",
    "            if pi_temp_sum > 0:\n",
    "                pi_temp /= pi_temp_sum\n",
    "            else:\n",
    "                pi_temp = pi\n",
    "            return pi_temp\n",
    "\n",
    "    def _select_child(self, node:MCTSNode, total_N:float) -> Tuple[int, MCTSNode]:\n",
    "        # choose action maximizing Q + U\n",
    "        best_score = -1e9\n",
    "        best_action = None\n",
    "        best_child = None\n",
    "        for a, child in node.children.items():\n",
    "            Q = 0.0\n",
    "            if child.visit_count > 0:\n",
    "                Q = child.q_value()\n",
    "            U = self.c_puct * child.prior * math.sqrt(total_N) / (1 + child.visit_count + child.virtual_loss)\n",
    "            score = Q + U\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_action = a\n",
    "                best_child = child\n",
    "        assert best_action is not None\n",
    "        return best_action, best_child\n",
    "\n",
    "    def _simulate(self, board:Board, to_move:int, root:MCTSNode):\n",
    "        start = time.perf_counter()\n",
    "        node = root\n",
    "        player = to_move\n",
    "        path: List[Tuple[MCTSNode, int]] = [] # nodes and actions\n",
    "\n",
    "        # selection\n",
    "        while True:\n",
    "            with node.lock:\n",
    "                if len(node.children) == 0: # leaf\n",
    "                    break\n",
    "                total_N = sum(ch.visit_count for ch in node.children.values())\n",
    "                action_idx, child = self._select_child(node, total_N)\n",
    "                child.virtual_loss += 1\n",
    "            # apply action\n",
    "            if action_idx == self.board_size*self.board_size:\n",
    "                mv = None\n",
    "            else:\n",
    "                x = action_idx % self.board_size\n",
    "                y = action_idx // self.board_size\n",
    "                mv = (x,y)\n",
    "            board._play_move_no_checks(player, mv)\n",
    "            path.append((node, action_idx))\n",
    "            node = child\n",
    "            player = opponent(player)\n",
    "\n",
    "        # Evaluate leaf with neural network\n",
    "        if board.game_over():\n",
    "            # terminal: value is +1 for winner from viewpoint of last player who moved\n",
    "            winner = board.winner()\n",
    "            value = 1.0 if winner == opponent(player) else -1.0\n",
    "        else:\n",
    "            t0 = time.perf_counter()\n",
    "            state = torch.tensor(board.get_state_planes(player), dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                logp, v = self.net(state.to(self.device))\n",
    "            probs = torch.exp(logp).squeeze(0).cpu().numpy()\n",
    "            self.prof['nn'] += time.perf_counter() - t0\n",
    "            # expand node with legal moves\n",
    "            legal = board.legal_moves(player)\n",
    "            legal_idxs = []\n",
    "            for mv in legal:\n",
    "                if mv is None:\n",
    "                    idx = self.board_size*self.board_size\n",
    "                else:\n",
    "                    idx = mv[1]*self.board_size + mv[0]\n",
    "                legal_idxs.append(idx)\n",
    "            # mask and renormalize probs for this leaf\n",
    "            masked = np.zeros_like(probs, dtype=np.float32)\n",
    "            masked[legal_idxs] = probs[legal_idxs]\n",
    "            s = masked.sum()\n",
    "            if s > 0:\n",
    "                masked /= s\n",
    "            else:\n",
    "                for idx in legal_idxs:\n",
    "                    masked[idx] = 1.0 / len(legal_idxs)\n",
    "            # create children for the leaf node\n",
    "            for idx in legal_idxs:\n",
    "                if idx not in node.children:\n",
    "                    node.children[idx] = MCTSNode(prior=float(masked[idx]))\n",
    "            value = float(v.item())\n",
    "        \n",
    "        # early termination heuristic: if abs(value) is extremely close to 1 and node depth small, backpropagate\n",
    "        # backup\n",
    "        cur_value = value\n",
    "        for parent, action_idx in reversed(path):\n",
    "            # remove virtual loss and update stats\n",
    "            child = parent.children[action_idx]\n",
    "            with parent.lock:\n",
    "                # decrement virtual loss applied during selection\n",
    "                if child.virtual_loss > 0:\n",
    "                    child.virtual_loss -= 1\n",
    "                parent.visit_count += 1\n",
    "                parent.value_sum += cur_value\n",
    "            cur_value = -cur_value\n",
    "\n",
    "\n",
    "        # if path empty we still may need to update root if leaf was root\n",
    "        if len(path) == 0:\n",
    "            # we're at root leaf; update root directly\n",
    "            with root.lock:\n",
    "                root.visit_count += 1\n",
    "                root.value_sum += cur_value\n",
    "        self.prof['sim'] += time.perf_counter() - start\n",
    "\n",
    "    # utility to print profiling\n",
    "    def print_profile(self):\n",
    "        print(\"MCTS profile:\")\n",
    "        for k, v in self.prof.items():\n",
    "            print(f\" {k}: {v:.4f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a028b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4) Replay buffer and self-play\n",
    "# ---------------------------\n",
    "Example = collections.namedtuple('Example', ['state', 'pi', 'z'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, example:Example):\n",
    "        self.buffer.append(example)\n",
    "\n",
    "    def push_many(self, examples):\n",
    "        for ex in examples:\n",
    "            self.buffer.append(ex)\n",
    "\n",
    "    def sample(self, batch_size:int, return_torch=False, device='cpu'):\n",
    "        if len(self.buffer) == 0:\n",
    "            raise ValueError('ReplayBuffer is empty')\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        states = np.stack([e.state for e in batch], axis=0)\n",
    "        pis = np.stack([e.pi for e in batch], axis=0)\n",
    "        zs = np.array([e.z for e in batch], dtype=np.float32)\n",
    "\n",
    "        if return_torch:\n",
    "            return (\n",
    "                torch.tensor(states, dtype=torch.float32, device=device),\n",
    "                torch.tensor(pis, dtype=torch.float32, device=device),\n",
    "                torch.tensor(zs, dtype=torch.float32, device=device),\n",
    "            )\n",
    "        return states, pis, zs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def self_play_episode(net:AlphaNet, mcts:MCTS, board_size:int, n_simulations:int, temperature:float=1.0):\n",
    "    board = Board(size=board_size)\n",
    "    to_move = BLACK\n",
    "    examples:List[Example] = []\n",
    "    move_no = 0\n",
    "    for move_no in range(board_size * board_size * 4):\n",
    "        if board.game_over():\n",
    "            break\n",
    "        pi = mcts.run(board, to_move)  # uses net internally\n",
    "        # safety: если MCTS вернул нули (не должно быть, но возможная защита)\n",
    "        s = pi.sum()\n",
    "        if s <= 0:\n",
    "            pi = np.ones_like(pi, dtype=np.float32)\n",
    "            pi /= pi.size\n",
    "        # apply temperature: simple softmax on visit counts (here pi already normalized)\n",
    "        if temperature == 0.0:\n",
    "            # pick argmax\n",
    "            a = int(np.argmax(pi))\n",
    "        else:\n",
    "            p = pi ** (1.0 / temperature)\n",
    "            p = p / (p.sum() + 1e-12)\n",
    "            a = int(np.random.choice(len(p), p=p))\n",
    "        # store example: state plane for current player, policy pi (from MCTS), z unknown yet\n",
    "        state = board.get_state_planes(to_move)\n",
    "        examples.append(Example(state=state, pi=pi.copy(), z=None))\n",
    "        # convert a to move\n",
    "        if a == board_size*board_size:\n",
    "            mv = None\n",
    "        else:\n",
    "            mv = (a % board_size, a // board_size)\n",
    "        if not board.is_legal(to_move, mv):\n",
    "            # forced pass if no legal moves\n",
    "            mv = None\n",
    "        board._play_move_no_checks(to_move, mv)\n",
    "        to_move = opponent(to_move)\n",
    "    # game finished: compute z for each example (from perspective of player to move at that state)\n",
    "    winner = board.winner()\n",
    "    if winner is None:\n",
    "        winner = BLACK\n",
    "    \n",
    "    # final scoring of examples\n",
    "    for ex in examples:\n",
    "        # if winner == BLACK -> z=+1 for states where to_move was BLACK, else -1\n",
    "        # but ex.state includes turn_plane indicating player at that state: turn_plane=1.0 means black to move\n",
    "        turn_plane = ex.state[-1]\n",
    "        black_to_move = bool(turn_plane[0, 0] > 0.5)\n",
    "        if winner == BLACK:\n",
    "            z = 1.0 if black_to_move else -1.0\n",
    "        else:\n",
    "            z = -1.0 if black_to_move else 1.0  # if white won, z=+1 for white-to-move states\n",
    "        # replace example with z\n",
    "        ex_z = Example(state=ex.state, pi=ex.pi, z=z)\n",
    "        yield ex_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b979fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5) Training loop (PyTorch)\n",
    "# ---------------------------\n",
    "import os\n",
    "\n",
    "# гиперпараметры по умолчанию, можно поменять\n",
    "TRAIN_PARAMS = {\n",
    "    \"batch_size\": 256,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"epochs_per_iteration\": 1,    # сколько шагов обучения за одну итерацию self-play\n",
    "    \"self_play_games\": 100,       # сколько партий собрать до обучения (на CPU может быть меньше)\n",
    "    \"replay_capacity\": 200000,\n",
    "    \"n_simulations\": 160,         # MCTS sim per move\n",
    "    \"c_puct\": 1.5,\n",
    "    \"dirichlet_alpha\": 0.3,\n",
    "    \"dirichlet_eps\": 0.25,\n",
    "    \"temperature\": 1.0,\n",
    "    \"save_dir\": \"./checkpoints\",\n",
    "    \"checkpoint_every\": 1,        # сохранять каждая итерация\n",
    "    \"eval_games\": 40,\n",
    "    \"num_iters\": 1000,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"device\": (\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "# util: ensure dir\n",
    "def ensure_dir(d):\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# loss: policy cross entropy (negative log likelihood) + value MSE\n",
    "# weight for value loss:\n",
    "VALUE_LOSS_WEIGHT = 0.7\n",
    "\n",
    "def train_step(net: AlphaNet, optimizer, replay: ReplayBuffer, batch_size: int, device: str):\n",
    "    # sample batch and run one optimizer step\n",
    "    states_np, pis_np, zs_np = replay.sample(batch_size)\n",
    "    # convert to tensors\n",
    "    states = torch.tensor(states_np, dtype=torch.float32, device=device)\n",
    "    pis = torch.tensor(pis_np, dtype=torch.float32, device=device)\n",
    "    zs = torch.tensor(zs_np, dtype=torch.float32, device=device)\n",
    "\n",
    "    print(\"len(replay) =\", len(replay))\n",
    "\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    # compute legal mask if you want; for now assume pis already zeros for illegal moves\n",
    "    logp, value = net(states)   # logp: [B, A], value: [B]\n",
    "    # policy loss: cross entropy with target pi (we use -sum pi * logp)\n",
    "    # note: if some pi entries are zero it's fine\n",
    "    policy_loss = - (pis * logp).sum(dim=1).mean()\n",
    "    value_loss = (value - zs).pow(2).mean()\n",
    "    loss = policy_loss + VALUE_LOSS_WEIGHT * value_loss\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(net.parameters(), TRAIN_PARAMS['grad_clip'])\n",
    "    optimizer.step()\n",
    "    return {\n",
    "        \"loss\": float(loss.item()),\n",
    "        \"policy_loss\": float(policy_loss.item()),\n",
    "        \"value_loss\": float(value_loss.item())\n",
    "    }\n",
    "\n",
    "# optional: simple prioritized replay wrapper (sum-tree not implemented here; lightweight version)\n",
    "class PrioritizedReplay:\n",
    "    # lightweight: store (example, priority), sample weighted by priority\n",
    "    def __init__(self, capacity=100000, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.buffer = []\n",
    "        self.pos = 0\n",
    "        self.max_prio = 1.0\n",
    "\n",
    "    def push(self, example):\n",
    "        prio = self.max_prio\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((example, prio))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (example, prio)\n",
    "            self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == 0:\n",
    "            raise ValueError(\"empty buffer\")\n",
    "        prios = np.array([p for (_, p) in self.buffer], dtype=np.float32)\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        idxs = np.random.choice(len(self.buffer), size=min(batch_size, len(self.buffer)), p=probs, replace=False)\n",
    "        batch = [self.buffer[i][0] for i in idxs]\n",
    "        # return also indices so caller can update priorities\n",
    "        return batch, idxs\n",
    "\n",
    "    def update_priorities(self, idxs, priorities):\n",
    "        for i, pr in zip(idxs, priorities):\n",
    "            self.buffer[i] = (self.buffer[i][0], pr)\n",
    "            self.max_prio = max(self.max_prio, pr)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# self-play worker sketch using multiprocessing.Process\n",
    "# it runs games and writes Examples to a queue for the trainer to read and insert into replay\n",
    "import multiprocessing as mp\n",
    "\n",
    "def self_play_worker(proc_id: int, net: AlphaNet, queue: mp.Queue, params: dict, stop_event: mp.Event):\n",
    "    # local MCTS uses a copy of network (weights are read-only here)\n",
    "    local_mcts = MCTS(net, board_size=net.board_size, c_puct=params[\"c_puct\"],\n",
    "                      n_simulations=params[\"n_simulations\"], device=params[\"device\"])\n",
    "    random_seed = int(time.time()) + proc_id * 1000\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    while not stop_event.is_set():\n",
    "        # play single episode\n",
    "        for ex in self_play_episode(net, local_mcts, net.board_size, params[\"n_simulations\"], temperature=params[\"temperature\"]):\n",
    "            # push to queue as raw Example\n",
    "            queue.put(ex)\n",
    "        # small sleep to yield CPU\n",
    "        time.sleep(0.01)\n",
    "    return\n",
    "\n",
    "# main training loop\n",
    "def train_loop(net: AlphaNet, params: dict):\n",
    "    device = params[\"device\"]\n",
    "    net.to(device)\n",
    "    # optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=params[\"lr\"], weight_decay=params[\"weight_decay\"])\n",
    "    # optional scheduler, e.g. cosine or step LR\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000, eta_min=1e-5)\n",
    "\n",
    "    # replay buffer\n",
    "    replay = ReplayBuffer(capacity=params[\"replay_capacity\"])\n",
    "    # or prioritized: \n",
    "    # replay = PrioritizedReplay(capacity=params[\"replay_capacity\"])\n",
    "\n",
    "    ensure_dir(params[\"save_dir\"])\n",
    "\n",
    "    # queue for self-play workers\n",
    "    manager = mp.Manager()\n",
    "    queue = manager.Queue(maxsize=10000)\n",
    "    stop_event = manager.Event()\n",
    "\n",
    "    # spawn workers\n",
    "    n_workers = max(1, mp.cpu_count() - 1)\n",
    "    workers = []\n",
    "    for i in range(n_workers):\n",
    "        # send net state dict to worker via deepcopy to avoid heavy IPC; worker uses same net object if in-process\n",
    "        p = mp.Process(target=self_play_worker, args=(i, net, queue, params, stop_event))\n",
    "        p.start()\n",
    "        workers.append(p)\n",
    "\n",
    "    try:\n",
    "        iter_no = 0\n",
    "        while iter_no < params[\"num_iters\"]:\n",
    "            if iter_no > 15:\n",
    "                fe = int(input())\n",
    "                if fe != \"\":\n",
    "                    break\n",
    "            # collect self-play games until we have enough\n",
    "            collected = 0\n",
    "            target_games = params[\"self_play_games\"]\n",
    "            while collected < target_games:\n",
    "                try:\n",
    "                    ex = queue.get(timeout=5.0)  # block waiting for new example\n",
    "                except Exception:\n",
    "                    # timeout or empty queue; can continue waiting\n",
    "                    continue\n",
    "                replay.push(ex)\n",
    "                # you might want to group examples by game to ensure episode integrity\n",
    "                collected += 1  # this counts examples; if you prefer games, change producer to signal game boundaries\n",
    "                # print progress occasionally\n",
    "                if len(replay) % 1000 == 0:\n",
    "                    print(f\"Replay size: {len(replay)}\")\n",
    "            # train for some epochs / steps\n",
    "            for ep in range(params[\"epochs_per_iteration\"]):\n",
    "                # number of gradient steps per epoch can be set relative to self_play_games\n",
    "                steps = max(1, len(replay) // params[\"batch_size\"])\n",
    "                for step in range(steps):\n",
    "                    stats = train_step(net, optimizer, replay, params[\"batch_size\"], device)\n",
    "                    if step % 10 == 0:\n",
    "                        print(f\"Iter {iter_no} Ep {ep} Step {step} loss {stats['loss']:.4f}\")\n",
    "                scheduler.step()\n",
    "\n",
    "            # save checkpoint\n",
    "            if iter_no % params[\"checkpoint_every\"] == 0:\n",
    "                ckpt = {\n",
    "                    \"iter\": iter_no,\n",
    "                    \"net_state\": net.state_dict(),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"replay_size\": len(replay),\n",
    "                    \"params\": params,\n",
    "                }\n",
    "                path = os.path.join(params[\"save_dir\"], f\"checkpoint_{iter_no}.pt\")\n",
    "                torch.save(ckpt, path)\n",
    "                print(\"Saved checkpoint\", path)\n",
    "\n",
    "            iter_no += 1\n",
    "\n",
    "    finally:\n",
    "        # stop workers\n",
    "        stop_event.set()\n",
    "        for p in workers:\n",
    "            p.terminate()\n",
    "            p.join()\n",
    "\n",
    "def train_loop_debug(\n",
    "    board_size=9,\n",
    "    n_iterations=20,\n",
    "    self_play_games_per_iter=5,\n",
    "    mcts_simulations=64,\n",
    "    batch_size=64,\n",
    "    epochs=1,\n",
    "    device=\"cpu\",\n",
    "    seed=0,\n",
    "):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = torch.device(device)\n",
    "\n",
    "    net = AlphaNet(board_size=board_size, num_res=6, channels=128).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    replay = ReplayBuffer(capacity=50000)\n",
    "\n",
    "    for it in range(n_iterations):\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Iteration\", it)\n",
    "\n",
    "        # -------------------------\n",
    "        # 1) self-play\n",
    "        # -------------------------\n",
    "        t0 = time.time()\n",
    "        total_moves = 0\n",
    "\n",
    "        for g in range(self_play_games_per_iter):\n",
    "            mcts = MCTS(\n",
    "                net,\n",
    "                board_size=board_size,\n",
    "                c_puct=1.5,\n",
    "                n_simulations=mcts_simulations,\n",
    "                device=device,\n",
    "            )\n",
    "            traj = list(self_play_episode(\n",
    "                net, mcts, board_size,\n",
    "                mcts_simulations, temperature=1.0\n",
    "            ))\n",
    "            for ex in traj:\n",
    "                replay.push(ex)\n",
    "            total_moves += len(traj)\n",
    "\n",
    "        t1 = time.time()\n",
    "        sp_time = t1 - t0\n",
    "        sp_speed = total_moves / max(sp_time, 1e-9)\n",
    "\n",
    "        print(f\"Self-play: {self_play_games_per_iter} games\")\n",
    "        print(f\"Moves generated: {total_moves}\")\n",
    "        print(f\"Self-play time: {sp_time:.2f} sec ({sp_speed:.1f} moves/sec)\")\n",
    "        print(f\"Replay size: {len(replay)}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 2) training\n",
    "        # -------------------------\n",
    "        if len(replay) < batch_size:\n",
    "            print(\"Not enough data in buffer, skipping training\")\n",
    "            continue\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        loss_list = []\n",
    "        policy_list = []\n",
    "        value_list = []\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            # number of steps per epoch\n",
    "            steps = max(1, len(replay) // batch_size)\n",
    "            for _ in range(steps):\n",
    "                states, pis, zs = replay.sample(batch_size)\n",
    "                states_t = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "                pis_t = torch.tensor(pis, dtype=torch.float32, device=device)\n",
    "                zs_t = torch.tensor(zs, dtype=torch.float32, device=device)\n",
    "\n",
    "                net.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                logp, v = net(states_t)\n",
    "\n",
    "                policy_loss = - (pis_t * logp).sum(dim=1).mean()\n",
    "                value_loss = ((v - zs_t) ** 2).mean()\n",
    "                loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_list.append(loss.item())\n",
    "                policy_list.append(policy_loss.item())\n",
    "                value_list.append(value_loss.item())\n",
    "\n",
    "        t3 = time.time()\n",
    "        train_time = t3 - t2\n",
    "\n",
    "        print(f\"Train time: {train_time:.2f} sec\")\n",
    "        print(f\"Avg loss:   {np.mean(loss_list):.4f}\")\n",
    "        print(f\"Avg policy: {np.mean(policy_list):.4f}\")\n",
    "        print(f\"Avg value:  {np.mean(value_list):.4f}\")\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "# simple evaluate function: play net vs current best policy (self play with fixed temperature=0)\n",
    "def evaluate(net: AlphaNet, opponent_net: AlphaNet, games: int = 40):\n",
    "    # plays games alternating colors\n",
    "    wins = 0\n",
    "    for g in range(games):\n",
    "        board = Board(size=net.board_size)\n",
    "        to_move = BLACK if (g % 2 == 0) else WHITE\n",
    "        if g % 2 == 0:\n",
    "            nets = {BLACK: net, WHITE: opponent_net}\n",
    "        else:\n",
    "            nets = {BLACK: opponent_net, WHITE: net}\n",
    "        mcts_players = {p: MCTS(nets[p], board_size=net.board_size, c_puct=1.5, n_simulations=160, device=TRAIN_PARAMS[\"device\"]) for p in (BLACK, WHITE)}\n",
    "        move_no = 0\n",
    "        while not board.game_over() and move_no < net.board_size * net.board_size * 4:\n",
    "            cur_net = nets[to_move]\n",
    "            mcts = mcts_players[to_move]\n",
    "            pi = mcts.run(board, to_move)\n",
    "            a = int(np.argmax(pi))\n",
    "            if a == net.board_size * net.board_size:\n",
    "                mv = None\n",
    "            else:\n",
    "                mv = (a % net.board_size, a // net.board_size)\n",
    "            board._play_move_no_checks(to_move, mv)\n",
    "            to_move = opponent(to_move)\n",
    "            move_no += 1\n",
    "        winner = board.winner()\n",
    "        if winner == (BLACK if (g % 2 == 0) else WHITE):\n",
    "            wins += 1\n",
    "    print(f\"Evaluation: {wins}/{games} wins\")\n",
    "\n",
    "# Example of running:\n",
    "# net = AlphaNet(board_size=9, in_channels=18, num_res=6, channels=128)\n",
    "# train_loop(net, TRAIN_PARAMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f46924b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Device: cpu\n",
      "==================================================\n",
      "Iteration 0\n",
      "Self-play: 4 games\n",
      "Moves generated: 680\n",
      "Self-play time: 234.52 sec (2.9 moves/sec)\n",
      "Replay size: 680\n",
      "Train time: 5.77 sec\n",
      "Avg loss:   4.3147\n",
      "Avg policy: 4.2799\n",
      "Avg value:  0.0697\n",
      "==================================================\n",
      "Iteration 1\n",
      "Self-play: 4 games\n",
      "Moves generated: 722\n",
      "Self-play time: 247.48 sec (2.9 moves/sec)\n",
      "Replay size: 1402\n",
      "Train time: 12.54 sec\n",
      "Avg loss:   4.8324\n",
      "Avg policy: 4.2900\n",
      "Avg value:  1.0848\n",
      "==================================================\n",
      "Iteration 2\n",
      "Self-play: 4 games\n",
      "Moves generated: 709\n",
      "Self-play time: 233.24 sec (3.0 moves/sec)\n",
      "Replay size: 2111\n",
      "Train time: 17.77 sec\n",
      "Avg loss:   5.0897\n",
      "Avg policy: 4.2958\n",
      "Avg value:  1.5879\n",
      "==================================================\n",
      "Iteration 3\n",
      "Self-play: 4 games\n",
      "Moves generated: 896\n",
      "Self-play time: 299.74 sec (3.0 moves/sec)\n",
      "Replay size: 3007\n",
      "Train time: 25.76 sec\n",
      "Avg loss:   4.8049\n",
      "Avg policy: 4.2947\n",
      "Avg value:  1.0204\n",
      "==================================================\n",
      "Iteration 4\n",
      "Self-play: 4 games\n",
      "Moves generated: 1005\n",
      "Self-play time: 341.57 sec (2.9 moves/sec)\n",
      "Replay size: 4012\n",
      "Train time: 34.51 sec\n",
      "Avg loss:   4.7983\n",
      "Avg policy: 4.3048\n",
      "Avg value:  0.9871\n",
      "==================================================\n",
      "Iteration 5\n",
      "Self-play: 4 games\n",
      "Moves generated: 898\n",
      "Self-play time: 291.99 sec (3.1 moves/sec)\n",
      "Replay size: 4910\n",
      "Train time: 42.70 sec\n",
      "Avg loss:   4.7924\n",
      "Avg policy: 4.2987\n",
      "Avg value:  0.9875\n",
      "==================================================\n",
      "Iteration 6\n",
      "Self-play: 4 games\n",
      "Moves generated: 873\n",
      "Self-play time: 284.72 sec (3.1 moves/sec)\n",
      "Replay size: 5783\n",
      "Train time: 49.73 sec\n",
      "Avg loss:   4.7905\n",
      "Avg policy: 4.3026\n",
      "Avg value:  0.9759\n",
      "==================================================\n",
      "Iteration 7\n",
      "Self-play: 4 games\n",
      "Moves generated: 715\n",
      "Self-play time: 229.25 sec (3.1 moves/sec)\n",
      "Replay size: 6498\n",
      "Train time: 55.50 sec\n",
      "Avg loss:   4.7889\n",
      "Avg policy: 4.2956\n",
      "Avg value:  0.9867\n",
      "==================================================\n",
      "Iteration 8\n",
      "Self-play: 4 games\n",
      "Moves generated: 876\n",
      "Self-play time: 282.46 sec (3.1 moves/sec)\n",
      "Replay size: 7374\n",
      "Train time: 62.76 sec\n",
      "Avg loss:   4.7912\n",
      "Avg policy: 4.2970\n",
      "Avg value:  0.9884\n",
      "==================================================\n",
      "Iteration 9\n",
      "Self-play: 4 games\n",
      "Moves generated: 689\n",
      "Self-play time: 228.15 sec (3.0 moves/sec)\n",
      "Replay size: 8063\n",
      "Train time: 69.84 sec\n",
      "Avg loss:   4.7876\n",
      "Avg policy: 4.2905\n",
      "Avg value:  0.9942\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 6) Training runner\n",
    "# ---------------------------\n",
    "\n",
    "def run_training():\n",
    "    # параметры можешь менять свободно\n",
    "    board_size = 9\n",
    "    n_iterations = 10\n",
    "    self_play_games_per_iter = 4\n",
    "    mcts_simulations = 48\n",
    "    batch_size = 64\n",
    "    epochs = 2\n",
    "    device = 'cpu'\n",
    "    seed = 0\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # если используешь debug-версию тренировки:\n",
    "    net = train_loop_debug(\n",
    "        board_size=board_size,\n",
    "        n_iterations=n_iterations,\n",
    "        self_play_games_per_iter=self_play_games_per_iter,\n",
    "        mcts_simulations=mcts_simulations,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        device=device,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # если используешь обычную train_loop — замени строку выше на:\n",
    "    # net = train_loop(...)\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    return net\n",
    "\n",
    "\n",
    "net = run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aca5963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game finished. Close window to exit.\n",
      "Score Black(with komi): 81.0, White: 3.5\n",
      "Winner: BLACK\n"
     ]
    }
   ],
   "source": [
    "# watch_match.py  (или вставь в конец az_go.py)\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# опционально: pygame рендер\n",
    "try:\n",
    "    import pygame\n",
    "    PYGAME_OK = True\n",
    "except Exception:\n",
    "    PYGAME_OK = False\n",
    "\n",
    "def load_net(path: str, board_size: int, device='cpu'):\n",
    "    \"\"\"Создает AlphaNet и загружает веса из path (torch .pth/.pt).\"\"\"\n",
    "    net = AlphaNet(board_size=board_size, num_res=3, channels=64)\n",
    "    net.load_state_dict(torch.load(path, map_location=device))\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "def board_to_ascii(board: Board):\n",
    "    \"\"\"Возвращает строковое представление доски ('.' empty, 'X' black, 'O' white).\"\"\"\n",
    "    lines = []\n",
    "    for y in range(board.size):\n",
    "        row = []\n",
    "        for x in range(board.size):\n",
    "            v = board.get(x, y)\n",
    "            if v == EMPTY:\n",
    "                row.append('.')\n",
    "            elif v == BLACK:\n",
    "                row.append('X')\n",
    "            else:\n",
    "                row.append('O')\n",
    "        lines.append(''.join(row))\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "class PygameRenderer:\n",
    "    def __init__(self, board_size=9, cell_size=30, padding=20):\n",
    "        if not PYGAME_OK:\n",
    "            raise RuntimeError(\"pygame not available\")\n",
    "        self.board_size = board_size\n",
    "        self.cell_size = cell_size\n",
    "        self.padding = padding\n",
    "        self.width = board_size * cell_size + padding * 2\n",
    "        self.height = self.width\n",
    "        self.last_pass = False\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "        pygame.display.set_caption(\"Watch Match\")\n",
    "        self.font = pygame.font.SysFont(None, 24)\n",
    "\n",
    "    def draw(self, board: Board, to_move):\n",
    "        self.screen.fill((210, 170, 110))\n",
    "        PADDING = self.padding\n",
    "        CELL = self.cell_size\n",
    "        STONE_R = int(CELL * 0.4)\n",
    "        # grid\n",
    "        for i in range(self.board_size):\n",
    "            pygame.draw.line(\n",
    "                self.screen, (0,0,0),\n",
    "                (PADDING, PADDING + i*CELL),\n",
    "                (PADDING + (self.board_size-1)*CELL, PADDING + i*CELL), 1)\n",
    "            pygame.draw.line(\n",
    "                self.screen, (0,0,0),\n",
    "                (PADDING + i*CELL, PADDING),\n",
    "                (PADDING + i*CELL, PADDING + (self.board_size-1)*CELL), 1)\n",
    "        # hoshi\n",
    "        hoshi = [3, 9, 15]\n",
    "        for hx in hoshi:\n",
    "            for hy in hoshi:\n",
    "                if 0 <= hx < self.board_size and 0 <= hy < self.board_size:\n",
    "                    pygame.draw.circle(self.screen, (0,0,0),\n",
    "                        (PADDING + hx*CELL, PADDING + hy*CELL), max(2, int(CELL*0.08)))\n",
    "        # stones\n",
    "        for y in range(self.board_size):\n",
    "            for x in range(self.board_size):\n",
    "                v = board.get(x,y)\n",
    "                if v == BLACK:\n",
    "                    pygame.draw.circle(self.screen, (0,0,0),\n",
    "                        (PADDING + x*CELL, PADDING + y*CELL), STONE_R)\n",
    "                elif v == WHITE:\n",
    "                    pygame.draw.circle(self.screen, (255,255,255),\n",
    "                        (PADDING + x*CELL, PADDING + y*CELL), STONE_R)\n",
    "        # turn text\n",
    "        txt = \"Black to move\" if to_move == BLACK else \"White to move\"\n",
    "        if self.last_pass: txt = \"White passed\" if to_move == BLACK else \"Black passed\"\n",
    "        surf = self.font.render(txt, True, (0,0,0))\n",
    "        self.screen.blit(surf, (5,5))\n",
    "        pygame.display.flip()\n",
    "\n",
    "def pick_action_from_pi(pi: np.ndarray, board: Board, board_size:int, temperature:float=0.0):\n",
    "    \"\"\"pi is visit-probabilities (length board_size*board_size+1). Returns chosen action index.\"\"\"\n",
    "    # mask illegal moves\n",
    "    current_player = board.turn\n",
    "    legal = board.legal_moves(board.turn)  # we will set current_player outside, but easier pass as global? avoid\n",
    "    # Better: build legal mask here by checking each index\n",
    "    mask = np.zeros_like(pi, dtype=np.bool_)\n",
    "    for mv in board.legal_moves(current_player):\n",
    "        if mv is None:\n",
    "            idx = board_size*board_size\n",
    "        else:\n",
    "            idx = mv[1]*board_size + mv[0]\n",
    "        mask[idx] = True\n",
    "    masked = np.copy(pi)\n",
    "    masked[~mask] = 0.0\n",
    "    s = masked.sum()\n",
    "    if s <= 0:\n",
    "        # fallback: uniform over legal\n",
    "        legal_idxs = np.where(mask)[0]\n",
    "        return int(np.random.choice(legal_idxs))\n",
    "    masked = masked / s\n",
    "    if temperature == 0.0:\n",
    "        return int(np.argmax(masked))\n",
    "    else:\n",
    "        p = masked ** (1.0 / temperature)\n",
    "        p = p / p.sum()\n",
    "        return int(np.random.choice(len(p), p=p))\n",
    "\n",
    "def play_match(net1: AlphaNet,\n",
    "               net2: AlphaNet,\n",
    "               board_size:int=9,\n",
    "               mcts_simulations:int=160,\n",
    "               c_puct:float=1.2,\n",
    "               device='cpu',\n",
    "               render: str = 'console',  # 'console' or 'pygame'\n",
    "               pause_time: float = 0.15,\n",
    "               temperature: float = 0.0):\n",
    "    \"\"\"\n",
    "    Провести партию между net1 (играет Black) и net2 (White).\n",
    "    render = 'console' или 'pygame'. pause_time в секундах между ходами.\n",
    "    \"\"\"\n",
    "    # подготовка MCTS для каждой сети\n",
    "    mcts1 = MCTS(net1, board_size=board_size, c_puct=c_puct, n_simulations=mcts_simulations, device=device)\n",
    "    mcts2 = MCTS(net2, board_size=board_size, c_puct=c_puct, n_simulations=mcts_simulations, device=device)\n",
    "\n",
    "    board = Board(size=board_size)\n",
    "    to_move = BLACK\n",
    "    move_no = 0\n",
    "\n",
    "    renderer = None\n",
    "    if render == 'pygame':\n",
    "        if not PYGAME_OK:\n",
    "            print(\"pygame not available, falling back to console render\")\n",
    "            render = 'console'\n",
    "        else:\n",
    "            renderer = PygameRenderer(board_size=board_size, cell_size=30, padding=30)\n",
    "\n",
    "    # main loop\n",
    "    while not board.game_over() and move_no < board_size*board_size*4:\n",
    "        if render == 'console':\n",
    "            print(\"\\nMove\", move_no, \"to_move:\", \"BLACK\" if to_move==BLACK else \"WHITE\")\n",
    "            print(board_to_ascii(board))\n",
    "        elif render == 'pygame':\n",
    "            renderer.draw(board, to_move)\n",
    "            # handle events to allow window close\n",
    "            for ev in pygame.event.get():\n",
    "                if ev.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "\n",
    "        # choose MCTS for current player\n",
    "        if to_move == BLACK:\n",
    "            pi = mcts1.run(board, to_move)  # visit-probs\n",
    "        else:\n",
    "            pi = mcts2.run(board, to_move)\n",
    "\n",
    "        # pick action deterministically if temperature==0\n",
    "        # convert pi to mask and argmax\n",
    "        # build legal mask and choose\n",
    "        size = board_size\n",
    "        mask = np.zeros_like(pi, dtype=np.bool_)\n",
    "        for mv in board.legal_moves(to_move):\n",
    "            if mv is None:\n",
    "                idx = size*size\n",
    "            else:\n",
    "                idx = mv[1]*size + mv[0]\n",
    "            mask[idx] = True\n",
    "        masked = np.copy(pi)\n",
    "        masked[~mask] = 0.0\n",
    "        if masked.sum() <= 0:\n",
    "            # fallback random legal\n",
    "            legal = board.legal_moves(to_move)\n",
    "            mv = random.choice(legal)\n",
    "        else:\n",
    "            if temperature == 0.0:\n",
    "                a = int(np.argmax(masked))\n",
    "            else:\n",
    "                p = masked ** (1.0 / temperature)\n",
    "                p = p / p.sum()\n",
    "                a = int(np.random.choice(len(p), p=p))\n",
    "            if a == size*size:\n",
    "                mv = None\n",
    "            else:\n",
    "                mv = (a % size, a // size)\n",
    "            renderer.last_pass = mv == None\n",
    "\n",
    "        # apply move (we use internal _play_move_no_checks to avoid extra superko checks done by is_legal,\n",
    "        # but to be safe, you can check is_legal before applying)\n",
    "        if mv is None:\n",
    "            board._play_move_no_checks(to_move, None)\n",
    "        else:\n",
    "            # ensure legal\n",
    "            if not board.is_legal(to_move, mv):\n",
    "                mv = None   # либо pass\n",
    "            board._play_move_no_checks(to_move, mv)\n",
    "\n",
    "        to_move = opponent(to_move)\n",
    "        move_no += 1\n",
    "        time.sleep(pause_time)\n",
    "\n",
    "    # finished\n",
    "    bscore, wscore = board.score_chinese()\n",
    "    winner = board.winner()\n",
    "    if render == 'console':\n",
    "        print(\"\\nFinal board:\")\n",
    "        print(board_to_ascii(board))\n",
    "        print(f\"Score Black: {bscore:.1f}, White(with komi): {wscore:.1f}\")\n",
    "        print(\"Winner:\", \"BLACK\" if winner==BLACK else \"WHITE\")\n",
    "    elif render == 'pygame':\n",
    "        renderer.draw(board, to_move)\n",
    "        print(\"Game finished. Close window to exit.\")\n",
    "        print(f\"Score Black(with komi): {bscore:.1f}, White: {wscore:.1f}\")\n",
    "        print(\"Winner:\", \"BLACK\" if winner==BLACK else \"WHITE\")\n",
    "        # keep window open until closed\n",
    "        while True:\n",
    "            for ev in pygame.event.get():\n",
    "                if ev.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "            time.sleep(0.1)\n",
    "\n",
    "# ---------------------------\n",
    "# Example usage:\n",
    "# ---------------------------\n",
    "# 1) если у тебя есть сохраненные веса:\n",
    "# net_black = load_net(\"best_black.pth\", board_size=9, device='cpu')\n",
    "# net_white = load_net(\"best_white.pth\", board_size=9, device='cpu')\n",
    "# play_match(net_black, net_white, board_size=9, mcts_simulations=128, render='console')\n",
    "\n",
    "# 2) если у тебя есть одна сеть (самонаблюдение), можно использовать одну и ту же модель:\n",
    "# net = load_net(\"model.pth\", board_size=9)\n",
    "# play_match(net, net, board_size=9, mcts_simulations=128, render='pygame', pause_time=0.4)\n",
    "\n",
    "# 3) если ты вызвал train_loop и получил net object:\n",
    "# net_trained = train_loop(...)\n",
    "# play_match(net_trained, net_trained, board_size=9, mcts_simulations=128, render='console')\n",
    "\n",
    "play_match(net, net, board_size=9, mcts_simulations=256, render='pygame', pause_time=0.25)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
